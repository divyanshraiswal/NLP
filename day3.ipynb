{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "434b021b-21b4-4511-9309-f04c019a8ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"\n",
    "Context\n",
    "The wikipedia dump is a giant XML file and contains loads of not-so-useful content. I needed some english text for some unsupervised learning so I spent quite a bit of time extracting and cleaning up the text.\n",
    "\n",
    "Content\n",
    "Each line of the txt file is a 'sentence'. I put sentence in quote because the content in these files haven't been read all the way through for errors. Here is what I did:\n",
    "\n",
    "Parsed out the opening text on non-disambiguation and non-table-of-contents pages.\n",
    "Removed sentences requiring citations, because these were usually poorly formed.\n",
    "Parse each block of text into sentences using SpaCy. I then checked for bracket and quote correctness, filtering out sentences that didn't quite match up.\n",
    "Removed sentences shorter than 3 letters and longer than 255 characters. This covers 97% of the data.\n",
    "Remove duplicate sentences, and, as a byproduct, sorted alphabetically.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b4068e5-d6fa-461d-99bc-f5526fbd22f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b32e616a-02a4-4925-a940-7c001115dee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nContext\\nThe wikipedia dump is a giant XML file and contains loads of not-so-useful content.', 'I needed some english text for some unsupervised learning so I spent quite a bit of time extracting and cleaning up the text.', \"Content\\nEach line of the txt file is a 'sentence'.\", \"I put sentence in quote because the content in these files haven't been read all the way through for errors.\", 'Here is what I did:\\n\\nParsed out the opening text on non-disambiguation and non-table-of-contents pages.', 'Removed sentences requiring citations, because these were usually poorly formed.', 'Parse each block of text into sentences using SpaCy.', \"I then checked for bracket and quote correctness, filtering out sentences that didn't quite match up.\", 'Removed sentences shorter than 3 letters and longer than 255 characters.', 'This covers 97% of the data.', 'Remove duplicate sentences, and, as a byproduct, sorted alphabetically.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80a8b462-7b8c-405e-b130-7424d2e09518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "corpus=[]\n",
    "\n",
    "for sentence in sentences:\n",
    "    text = re.sub('[^a-zA-Z]', ' ', sentence)  # Remove non-alphabetic characters\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text if word not in set(stopwords.words('english'))]\n",
    "    text = \" \".join(text)\n",
    "    corpus.append(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5da504ce-6335-4f4f-9730-9a8b9785e1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['context wikipedia dump giant xml file contains load useful content',\n",
       " 'needed english text unsupervised learning spent quite bit time extracting cleaning text',\n",
       " 'content line txt file sentence',\n",
       " 'put sentence quote content file read way error',\n",
       " 'parsed opening text non disambiguation non table content page',\n",
       " 'removed sentence requiring citation usually poorly formed',\n",
       " 'parse block text sentence using spacy',\n",
       " 'checked bracket quote correctness filtering sentence quite match',\n",
       " 'removed sentence shorter letter longer character',\n",
       " 'cover data',\n",
       " 'remove duplicate sentence byproduct sorted alphabetically']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5f1cc59-ddca-498d-a28f-9dcf4b3b5478",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "cv=CountVectorizer(binary=True,ngram_range=(2,3))   \n",
    "x=cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4091fae-f88d-40a3-83bb-c4dc595b0270",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context wikipedia': 20,\n",
       " 'wikipedia dump': 120,\n",
       " 'dump giant': 27,\n",
       " 'giant xml': 42,\n",
       " 'xml file': 122,\n",
       " 'file contains': 35,\n",
       " 'contains load': 13,\n",
       " 'load useful': 50,\n",
       " 'useful content': 115,\n",
       " 'context wikipedia dump': 21,\n",
       " 'wikipedia dump giant': 121,\n",
       " 'dump giant xml': 28,\n",
       " 'giant xml file': 43,\n",
       " 'xml file contains': 123,\n",
       " 'file contains load': 36,\n",
       " 'contains load useful': 14,\n",
       " 'load useful content': 51,\n",
       " 'needed english': 53,\n",
       " 'english text': 31,\n",
       " 'text unsupervised': 107,\n",
       " 'unsupervised learning': 113,\n",
       " 'learning spent': 44,\n",
       " 'spent quite': 99,\n",
       " 'quite bit': 68,\n",
       " 'bit time': 0,\n",
       " 'time extracting': 109,\n",
       " 'extracting cleaning': 33,\n",
       " 'cleaning text': 12,\n",
       " 'needed english text': 54,\n",
       " 'english text unsupervised': 32,\n",
       " 'text unsupervised learning': 108,\n",
       " 'unsupervised learning spent': 114,\n",
       " 'learning spent quite': 45,\n",
       " 'spent quite bit': 100,\n",
       " 'quite bit time': 69,\n",
       " 'bit time extracting': 1,\n",
       " 'time extracting cleaning': 110,\n",
       " 'extracting cleaning text': 34,\n",
       " 'content line': 17,\n",
       " 'line txt': 48,\n",
       " 'txt file': 111,\n",
       " 'file sentence': 39,\n",
       " 'content line txt': 18,\n",
       " 'line txt file': 49,\n",
       " 'txt file sentence': 112,\n",
       " 'put sentence': 66,\n",
       " 'sentence quote': 88,\n",
       " 'quote content': 71,\n",
       " 'content file': 15,\n",
       " 'file read': 37,\n",
       " 'read way': 75,\n",
       " 'way error': 119,\n",
       " 'put sentence quote': 67,\n",
       " 'sentence quote content': 89,\n",
       " 'quote content file': 72,\n",
       " 'content file read': 16,\n",
       " 'file read way': 38,\n",
       " 'read way error': 76,\n",
       " 'parsed opening': 63,\n",
       " 'opening text': 59,\n",
       " 'text non': 103,\n",
       " 'non disambiguation': 55,\n",
       " 'disambiguation non': 25,\n",
       " 'non table': 57,\n",
       " 'table content': 101,\n",
       " 'content page': 19,\n",
       " 'parsed opening text': 64,\n",
       " 'opening text non': 60,\n",
       " 'text non disambiguation': 104,\n",
       " 'non disambiguation non': 56,\n",
       " 'disambiguation non table': 26,\n",
       " 'non table content': 58,\n",
       " 'table content page': 102,\n",
       " 'removed sentence': 79,\n",
       " 'sentence requiring': 90,\n",
       " 'requiring citation': 82,\n",
       " 'citation usually': 10,\n",
       " 'usually poorly': 117,\n",
       " 'poorly formed': 65,\n",
       " 'removed sentence requiring': 80,\n",
       " 'sentence requiring citation': 91,\n",
       " 'requiring citation usually': 83,\n",
       " 'citation usually poorly': 11,\n",
       " 'usually poorly formed': 118,\n",
       " 'parse block': 61,\n",
       " 'block text': 2,\n",
       " 'text sentence': 105,\n",
       " 'sentence using': 94,\n",
       " 'using spacy': 116,\n",
       " 'parse block text': 62,\n",
       " 'block text sentence': 3,\n",
       " 'text sentence using': 106,\n",
       " 'sentence using spacy': 95,\n",
       " 'checked bracket': 8,\n",
       " 'bracket quote': 4,\n",
       " 'quote correctness': 73,\n",
       " 'correctness filtering': 22,\n",
       " 'filtering sentence': 40,\n",
       " 'sentence quite': 86,\n",
       " 'quite match': 70,\n",
       " 'checked bracket quote': 9,\n",
       " 'bracket quote correctness': 5,\n",
       " 'quote correctness filtering': 74,\n",
       " 'correctness filtering sentence': 23,\n",
       " 'filtering sentence quite': 41,\n",
       " 'sentence quite match': 87,\n",
       " 'sentence shorter': 92,\n",
       " 'shorter letter': 96,\n",
       " 'letter longer': 46,\n",
       " 'longer character': 52,\n",
       " 'removed sentence shorter': 81,\n",
       " 'sentence shorter letter': 93,\n",
       " 'shorter letter longer': 97,\n",
       " 'letter longer character': 47,\n",
       " 'cover data': 24,\n",
       " 'remove duplicate': 77,\n",
       " 'duplicate sentence': 29,\n",
       " 'sentence byproduct': 84,\n",
       " 'byproduct sorted': 6,\n",
       " 'sorted alphabetically': 98,\n",
       " 'remove duplicate sentence': 78,\n",
       " 'duplicate sentence byproduct': 30,\n",
       " 'sentence byproduct sorted': 85,\n",
       " 'byproduct sorted alphabetically': 7}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a3e36f7-1aef-47a1-904a-64f124e0d3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'context wikipedia dump giant xml file contains load useful content'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0] #first sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd41c8b2-0777-4b95-a93e-5f346d6dbfcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].toarray()# Bag of words for first sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd8ea3f3-fc91-44dd-9c35-954dd391a678",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TFIDF \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv=TfidfVectorizer(ngram_range=(2,3))\n",
    "x=cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2463d2ac-4fba-46d1-ab87-1c2cff7fa1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context wikipedia': 20,\n",
       " 'wikipedia dump': 120,\n",
       " 'dump giant': 27,\n",
       " 'giant xml': 42,\n",
       " 'xml file': 122,\n",
       " 'file contains': 35,\n",
       " 'contains load': 13,\n",
       " 'load useful': 50,\n",
       " 'useful content': 115,\n",
       " 'context wikipedia dump': 21,\n",
       " 'wikipedia dump giant': 121,\n",
       " 'dump giant xml': 28,\n",
       " 'giant xml file': 43,\n",
       " 'xml file contains': 123,\n",
       " 'file contains load': 36,\n",
       " 'contains load useful': 14,\n",
       " 'load useful content': 51,\n",
       " 'needed english': 53,\n",
       " 'english text': 31,\n",
       " 'text unsupervised': 107,\n",
       " 'unsupervised learning': 113,\n",
       " 'learning spent': 44,\n",
       " 'spent quite': 99,\n",
       " 'quite bit': 68,\n",
       " 'bit time': 0,\n",
       " 'time extracting': 109,\n",
       " 'extracting cleaning': 33,\n",
       " 'cleaning text': 12,\n",
       " 'needed english text': 54,\n",
       " 'english text unsupervised': 32,\n",
       " 'text unsupervised learning': 108,\n",
       " 'unsupervised learning spent': 114,\n",
       " 'learning spent quite': 45,\n",
       " 'spent quite bit': 100,\n",
       " 'quite bit time': 69,\n",
       " 'bit time extracting': 1,\n",
       " 'time extracting cleaning': 110,\n",
       " 'extracting cleaning text': 34,\n",
       " 'content line': 17,\n",
       " 'line txt': 48,\n",
       " 'txt file': 111,\n",
       " 'file sentence': 39,\n",
       " 'content line txt': 18,\n",
       " 'line txt file': 49,\n",
       " 'txt file sentence': 112,\n",
       " 'put sentence': 66,\n",
       " 'sentence quote': 88,\n",
       " 'quote content': 71,\n",
       " 'content file': 15,\n",
       " 'file read': 37,\n",
       " 'read way': 75,\n",
       " 'way error': 119,\n",
       " 'put sentence quote': 67,\n",
       " 'sentence quote content': 89,\n",
       " 'quote content file': 72,\n",
       " 'content file read': 16,\n",
       " 'file read way': 38,\n",
       " 'read way error': 76,\n",
       " 'parsed opening': 63,\n",
       " 'opening text': 59,\n",
       " 'text non': 103,\n",
       " 'non disambiguation': 55,\n",
       " 'disambiguation non': 25,\n",
       " 'non table': 57,\n",
       " 'table content': 101,\n",
       " 'content page': 19,\n",
       " 'parsed opening text': 64,\n",
       " 'opening text non': 60,\n",
       " 'text non disambiguation': 104,\n",
       " 'non disambiguation non': 56,\n",
       " 'disambiguation non table': 26,\n",
       " 'non table content': 58,\n",
       " 'table content page': 102,\n",
       " 'removed sentence': 79,\n",
       " 'sentence requiring': 90,\n",
       " 'requiring citation': 82,\n",
       " 'citation usually': 10,\n",
       " 'usually poorly': 117,\n",
       " 'poorly formed': 65,\n",
       " 'removed sentence requiring': 80,\n",
       " 'sentence requiring citation': 91,\n",
       " 'requiring citation usually': 83,\n",
       " 'citation usually poorly': 11,\n",
       " 'usually poorly formed': 118,\n",
       " 'parse block': 61,\n",
       " 'block text': 2,\n",
       " 'text sentence': 105,\n",
       " 'sentence using': 94,\n",
       " 'using spacy': 116,\n",
       " 'parse block text': 62,\n",
       " 'block text sentence': 3,\n",
       " 'text sentence using': 106,\n",
       " 'sentence using spacy': 95,\n",
       " 'checked bracket': 8,\n",
       " 'bracket quote': 4,\n",
       " 'quote correctness': 73,\n",
       " 'correctness filtering': 22,\n",
       " 'filtering sentence': 40,\n",
       " 'sentence quite': 86,\n",
       " 'quite match': 70,\n",
       " 'checked bracket quote': 9,\n",
       " 'bracket quote correctness': 5,\n",
       " 'quote correctness filtering': 74,\n",
       " 'correctness filtering sentence': 23,\n",
       " 'filtering sentence quite': 41,\n",
       " 'sentence quite match': 87,\n",
       " 'sentence shorter': 92,\n",
       " 'shorter letter': 96,\n",
       " 'letter longer': 46,\n",
       " 'longer character': 52,\n",
       " 'removed sentence shorter': 81,\n",
       " 'sentence shorter letter': 93,\n",
       " 'shorter letter longer': 97,\n",
       " 'letter longer character': 47,\n",
       " 'cover data': 24,\n",
       " 'remove duplicate': 77,\n",
       " 'duplicate sentence': 29,\n",
       " 'sentence byproduct': 84,\n",
       " 'byproduct sorted': 6,\n",
       " 'sorted alphabetically': 98,\n",
       " 'remove duplicate sentence': 78,\n",
       " 'duplicate sentence byproduct': 30,\n",
       " 'sentence byproduct sorted': 85,\n",
       " 'byproduct sorted alphabetically': 7}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c102d198-5112-42ca-905e-dca31f319ded",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.24253563, 0.24253563,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.24253563, 0.24253563, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.24253563, 0.24253563, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.24253563, 0.24253563, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.24253563, 0.24253563, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.24253563, 0.24253563, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.24253563, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.24253563, 0.24253563, 0.24253563, 0.24253563]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].toarray()# Bag of words for first sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d818d25b-f551-43d9-a000-77baf19087ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
